{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "950bab64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 67, 72, 76, 79]\n",
      "(87, 5000, 120)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "outfile = open('../data/Piano-midi.de.pickle','rb')\n",
    "data = pickle.load(outfile)\n",
    "print(data[\"train\"][4][19])\n",
    "outfile.close()\n",
    "from matplotlib import pyplot\n",
    "dataset=np.zeros((len(data[\"train\"]), 5000, 120))\n",
    "\n",
    "for i in range(len(data[\"train\"])): #sample\n",
    "    for j in range(len(data[\"train\"][i])): #second\n",
    "        for k in range(len(data[\"train\"][i][j])):\n",
    "            dataset[i, j, data[\"train\"][i][j][k]]=1; #note\n",
    "\n",
    "print(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0bfae4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##dataset = pickle.load('../data.Piano-midi.de.pickle')\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "#from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Conv1DTranspose\n",
    "\n",
    "def define_discriminator(in_shape):\n",
    "    model = Sequential()\n",
    "    # normal\n",
    "    \n",
    "    model.add(Conv1D(16, 2, padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv1D(64, 2, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv1D(128, 2, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv1D(256, 2, strides=2, padding='same'))   \n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0004, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(examples, n_examples):\n",
    "    # plot images\n",
    "\tfor i in range(n_examples):\n",
    "\t\t# define subplot\n",
    "\t\tpyplot.subplot(sqrt(n_examples), sqrt(n_examples), 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tpyplot.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\tpyplot.imshow(examples[i].astype('uint8'))\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "05492cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim, output_shape):\n",
    "    start_height = output_shape[0] // 8\n",
    "    start_width = output_shape[1] // 8\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    n_nodes = 32* start_width * start_height\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((4*start_height, 8*start_width)))\n",
    "    \n",
    "    model.add(Conv1DTranspose(240, 4, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((8*start_height, 16*start_width)))\n",
    "    \n",
    "    model.add(Conv1DTranspose(240, 4, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((16*start_height, 16*start_width)))\n",
    "    \n",
    "    model.add(Conv1DTranspose(128, 3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Conv1D(120, 2, activation='tanh', padding='same'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f05d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model, loss='binary_crossentropy', optimizer=None):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    if not optimizer:\n",
    "        optimizer = Adam(lr=0.0004, beta_1=0.5)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b484622f",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples, multiplier=1.0):\n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    # select images\n",
    "    X = dataset[ix]\n",
    "    # generate class labels, -1 for 'real'\n",
    "    y = np.ones((n_samples, 1)) * multiplier\n",
    "    return X, y\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples, multiplier=1.0):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels with 1.0 for 'fake'\n",
    "    y = np.ones((n_samples, 1)) * multiplier\n",
    "    return X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = np.random.randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(genenerator, discriminator, gan_model, dataset, latent_dim, output_path,\n",
    "          real_samples_multiplier=1.0, fake_samples_multiplier=0.0, discriminator_batches=1,\n",
    "          n_epochs=200, n_batch=128):\n",
    "    batch_per_epoch = dataset.shape[0] // n_batch\n",
    "    half_batch = n_batch // 2\n",
    "    seed = generate_latent_points(latent_dim, 54)\n",
    "    n_steps = batch_per_epoch * n_epochs\n",
    "\t\n",
    "    history = {'discriminator_real_loss': [],\n",
    "               'discriminator_fake_loss': [],\n",
    "               'generator_loss': []}\n",
    "    for step in range(n_steps):\n",
    "        epoch = step // batch_per_epoch\n",
    "        disc_loss_real = 0.0\n",
    "        disc_loss_fake = 0.0\n",
    "        for disc_batch in range(discriminator_batches):\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch,\n",
    "                                                   multiplier=real_samples_multiplier)\n",
    "            disc_loss_real += discriminator.train_on_batch(X_real, y_real)\n",
    "            X_fake, y_fake = generate_fake_samples(genenerator, latent_dim, half_batch,\n",
    "                                                   multiplier=fake_samples_multiplier)\n",
    "            disc_loss_fake += discriminator.train_on_batch(X_fake, y_fake)\n",
    "        disc_loss_real /= discriminator_batches\n",
    "        disc_loss_fake /= discriminator_batches\n",
    "        \n",
    "        X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        y_gan = np.ones((n_batch, 1)) * real_samples_multiplier\n",
    "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        \n",
    "        history['discriminator_real_loss'].append(disc_loss_real)\n",
    "        history['discriminator_fake_loss'].append(disc_loss_fake)\n",
    "        history['generator_loss'].append(g_loss)\n",
    "        epoch = step // batch_per_epoch+1\n",
    "        print('epoch: %d, discriminator_real_loss=%.3f, discriminator_fake_loss=%.3f, generator_loss=%.3f' % (epoch, disc_loss_real, disc_loss_fake, g_loss))\n",
    "\n",
    "    return history\n",
    "            \n",
    "    \n",
    "def save_models(generator, discriminator, output_path):\n",
    "    save_generator_path = f'{output_path}/generator_models'\n",
    "    save_discriminator_path = f'{output_path}/discriminator_models'\n",
    "    if not os.path.exists(save_generator_path):\n",
    "        os.makedirs(save_generator_path)\n",
    "    if not os.path.exists(save_discriminator_path):\n",
    "        os.makedirs(save_discriminator_path)\n",
    "    genenerator.save(save_generator_path + f'/generator_model_{epoch}.h5')\n",
    "    discriminator.save(save_discriminator_path + f'/discriminator_model_{epoch}.h5')\n",
    "    \n",
    "def save_images(epoch, n_cols, n_rows, seed, output_path, model, preview_margin=16, image_size=(32, 32)): \n",
    "    height = image_size[0]\n",
    "    width = image_size[1]\n",
    "    image_array = np.full((preview_margin + (n_rows * (height+preview_margin)), \n",
    "                           preview_margin + (n_cols * (width+preview_margin)), image_size[2]), \n",
    "                          255, dtype=np.uint8)\n",
    "                \n",
    "    generated_images = model.predict(seed)\n",
    "    generated_images = (generated_images + 1.0) * 127.5\n",
    "    generated_images = np.round(generated_images).astype(np.uint8)\n",
    "    if image_size[2] == 4:\n",
    "        generated_images[(generated_images[:, :, :, 3] == 127) | (generated_images[:, :, :, 3] == 128)] = 0\n",
    "\n",
    "\n",
    "    image_count = 0\n",
    "    for row in range(n_rows):\n",
    "        for col in range(n_cols):\n",
    "            r = row * (height+preview_margin) + preview_margin\n",
    "            c = col * (width+preview_margin) + preview_margin\n",
    "            image_array[r:r+height,c:c+width] = generated_images[image_count]\n",
    "            image_count += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "97abeb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 5000, 120) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5000, 120), dtype=tf.float32, name='conv1d_153_input'), name='conv1d_153_input', description=\"created by layer 'conv1d_153_input'\"), but it was called on an input with incompatible shape (None, 20000, 120).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential_87\" (type Sequential).\n\nInput 0 of layer \"dense_71\" is incompatible with the layer: expected axis -1of input shape to have value 160000, but received input with shape (None, 640000)\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 20000, 120), dtype=float32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15720/1120419819.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"xyz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15720/2233873881.py\u001b[0m in \u001b[0;36mdefine_gan\u001b[1;34m(g_model, d_model, loss, optimizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# add the discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;31m# compile model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Bootcamp2021\\bootcamp_env\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Bootcamp2021\\bootcamp_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Bootcamp2021\\bootcamp_env\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    245\u001b[0m           \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mshape_as_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m           raise ValueError(\n\u001b[0m\u001b[0;32m    248\u001b[0m               \u001b[1;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m               \u001b[1;34mf'incompatible with the layer: expected axis {axis}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_87\" (type Sequential).\n\nInput 0 of layer \"dense_71\" is incompatible with the layer: expected axis -1of input shape to have value 160000, but received input with shape (None, 640000)\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 20000, 120), dtype=float32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "dis = define_discriminator((5000, 120))\n",
    "gen = define_generator(50, (5000, 120))\n",
    "gan = define_gan(gen, dis)\n",
    "hist = train(gen, dis, gan, dataet, (500, 12), \"xyz\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
